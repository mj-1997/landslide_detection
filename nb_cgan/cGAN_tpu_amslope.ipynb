{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cGAN_tpu_amslope.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["hwdlxLDjFFAn"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9LlN8wgNNclE"},"source":["##Start up"]},{"cell_type":"markdown","metadata":{"id":"nG2A_pxD1fXN"},"source":["### GDrive"]},{"cell_type":"code","metadata":{"id":"HvA6uejaN4aA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-s8ZLiK1iGP"},"source":["### Enable TPU\n","Edit --> Notebook Settings --> TPU\n","Then check if TPU works"]},{"cell_type":"markdown","metadata":{"id":"8L_G9P3QHMFM"},"source":["Ensure that the number of instances is perfectly divisible by the steps_per_epoch parameter so that all the instances are used during\n","training. For example, we have 60000 instances in our training set. 60000\n","is divisible by 50 so that means all our instances are fed into the model without any leftovers. If you hit run, it should start training on a TPU instance after a short while:\n","\n","\n","Basically should have that data is 256 all the time in batch not a leftover with less... Could get rid of the data that does not fit into it?"]},{"cell_type":"code","metadata":{"id":"uNkSvTzz1tde"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xF0zrSmSfTO-"},"source":["#This strategy ssems to not work?\n","\n","# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"up3WB1dw1xnl"},"source":["### Import lib"]},{"cell_type":"code","metadata":{"id":"Zhi-IN5jWgwn"},"source":["#Genna\n","!pip install -q segmentation_models\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8v_0K5UScc4v"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VBCw6Pxchlz"},"source":["print (tf.version)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbpBqfskr2t1"},"source":["from segmentation_models.losses import binary_focal_dice_loss #this also exists.."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oH6wjh1ojF6"},"source":["from segmentation_models.losses import bce_dice_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_52JyrAHNhH2"},"source":["import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","# import tensorflow as tf\n","import pathlib\n","import re\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","# import itertools\n","import time\n","# from sklearn.metrics import confusion_matrix\n","# from sklearn.metrics import classification_report\n","# import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9g6c-bTchAic"},"source":["\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","%matplotlib inline\n","\n","from tqdm import tqdm_notebook, tnrange\n","from itertools import chain\n","from skimage.io import imread, imshow, concatenate_images\n","from skimage.transform import resize\n","from skimage.morphology import label\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","\n","from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n","from keras.layers.core import Lambda, RepeatVector, Reshape\n","from keras.layers.convolutional import Conv2D, Conv2DTranspose\n","from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n","from keras.layers.merge import concatenate, add\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","# from keras.optimizers import Adam #worked before since 19-08-2021 doesnt work anymore so now import via tf\n","from tensorflow.keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJQdQboBS7HD"},"source":["\n","from skimage.io import imread\n","from skimage.transform import resize\n","# from keras.models import Sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgDrqIr_mn7B"},"source":["from numpy import load\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randint\n","# from keras.optimizers import Adam\n","from keras.initializers import RandomNormal\n","from keras.models import Model\n","from keras.models import Input\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","from keras.layers import Dropout\n","from keras.layers import BatchNormalization\n","from keras.layers import LeakyReLU\n","# from matplotlib import pyplot\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OX8_6AzmyZCg"},"source":["## Choose properties"]},{"cell_type":"code","metadata":{"id":"9xpxFYkXjSlR"},"source":["# global parameters\n","P = {\n","\n","    'SEED': 42,\n","     \n","    # image size to sample\n","    # 'SIZE': 160, \n","    # 'SIZE': 224, \n","     'SIZE': 256, \n","    # 'SIZE': 320,\n","    # 'BATCH_SIZE': 32 * strategy.num_replicas_in_sync, #256.. 64 is 512. \n","    \n","    # 'FOLDS': 5,\n","    \n","    # 'LEARNING_RATE': 0.00075,\n","    \n","    'EPOCHS': 100,\n","     \n","    # 'VERSION': 'v6'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuohlkp76i0j"},"source":["# BATCH_SIZE_PER_REPLICA = 64\n","# GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","# tpu_strategy.num_replicas_in_sync #8.\n","# 256/8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0ZaEuh2ycCP"},"source":["n_batch = 64\n","num_epochs = 100 #200 in og pix2pix recommended\n","path_save_gan = \"/content/drive/MyDrive/Thesis/big_ls_augment_v3/temp_models/\"\n","# os.makedirs(path_save_gan)\n","#attempt 1; repeat(2)\n","#attempt 2; repeat(3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tl-9hGvZg4E0"},"source":["!ls -la {path_save_gan}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uao7QnxHyuTx"},"source":["path_results = \"/content/drive/MyDrive/Thesis/big_ls_augment_v3/Results/attempt_1_1409_bcedice_des_and_asc_bsize_\" + str(n_batch) + \"/\"\n","os.makedirs(path_results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6byztb7Eyr_b"},"source":["path_to_save = path_save_gan + \"attempt_1_1409_bcedice_des_and_asc_bsize_\" + str(n_batch) + \"/\" #hceck if works \n","os.makedirs(path_to_save) #seems as if this is not made..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xo1B97qumulL"},"source":["path_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwPLPIGLm31l"},"source":["path_to_save"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHM-Arpm-I3o"},"source":["def remove_files(dir):\n","  for f in os.listdir(dir):\n","      os.remove(os.path.join(dir, f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixs7bTI3-TaS"},"source":["# remove_files(path_to_save)\n","# remove_files(path_results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_s19EohvZpJR"},"source":["!ls -la {path_to_save}\n","!ls -la {path_results}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnasP0Na_kvs"},"source":["files from kaggle"]},{"cell_type":"code","metadata":{"id":"hR-32_xRbs22"},"source":["tfrec_files_all_ls = tf.io.gfile.glob('gs://kds-60e70aece3307e40b9e154943a40dcbdb493d3d10f4776c24b1d0174/*')\n","print('all ls files',tfrec_files_all_ls)\n","tfrec_files_all_ls_des = tfrec_files_all_ls[6:]\n","print('des ls files',tfrec_files_all_ls_des)\n","tfrec_files_all_ls_asc = tfrec_files_all_ls[:6]\n","print('asc ls files',tfrec_files_all_ls_asc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-8L8xSarfvM"},"source":["tfrec_files_ls = tf.io.gfile.glob('gs://kds-2fce72c5034cc086f74d9f3987cda7e253d4f468e128d93560b9a47b/*')\n","print('all ls files',tfrec_files_ls)\n","tfrec_files_ls_des = tfrec_files_ls[3:]\n","print('des ls files',tfrec_files_ls_des)\n","tfrec_files_ls_asc = tfrec_files_ls[:3]\n","print('des asc val files',tfrec_files_ls_asc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft2wZtwqNOgH"},"source":["tfrec_files_val_ls = tf.io.gfile.glob('gs://kds-edf2dfc9e536d4c9873bf35b19e5a84be62a4e2f30964f18939238dd/*')\n","print('all ls val files',tfrec_files_val_ls)\n","tfrec_files_val_ls_des =  tfrec_files_val_ls[2:]\n","print('des ls val files',tfrec_files_val_ls_des)\n","tfrec_files_val_ls_asc =  tfrec_files_val_ls[:2]\n","print('asc val files',tfrec_files_val_ls_asc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07YnnkRINOgH"},"source":["len_files_to_shuffle = (len(tfrec_files_ls_asc)) * 100 #the last one has something between one and hundred\n","len_files_to_shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eACO6VseNOgH"},"source":["len_files_val_to_shuffle = (len(tfrec_files_val_ls)) * 50 #the last one has something between one and hundred\n","len_files_val_to_shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNjCWByL3crG"},"source":["Can make the val dataset exactly as long as the batch size? such that there is no small remainder? No there are only 100..."]},{"cell_type":"markdown","metadata":{"id":"hwdlxLDjFFAn"},"source":["#CGAN\n","https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/"]},{"cell_type":"markdown","metadata":{"id":"j07yi4A19cH8"},"source":["Alternative ; https://www.tensorflow.org/tutorials/generative/pix2pix#training"]},{"cell_type":"markdown","metadata":{"id":"sQXsAnx0k6AM"},"source":["## Define cGAN"]},{"cell_type":"code","metadata":{"id":"os9ksgTRTnrl"},"source":["# define the discriminator model\n","def define_discriminator(image_shape,image_shape_target):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# source image input\n","\tin_src_image = Input(shape=image_shape)\n","\t# target image input\n","\tin_target_image = Input(shape=image_shape_target)\n","\t# concatenate images channel-wise\n","\tmerged = Concatenate()([in_src_image, in_target_image])\n","\t# C64\n","\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C128\n","\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = BatchNormalization()(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C256\n","\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = BatchNormalization()(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C512\n","\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = BatchNormalization()(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# second last output layer\n","\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","\td = BatchNormalization()(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# patch output\n","\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","\tpatch_out = Activation('sigmoid')(d)\n","\t# define model\n","\tmodel = Model([in_src_image, in_target_image], patch_out)\n","\t# compile model\n","\topt = Adam(learning_rate=0.0002, beta_1=0.5) #lr\n","  \n","  # opt = Adam(learning_rate=0.0002, beta_1=0.5)\n","\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n","  # model.compile(loss= binary_focal_dice_loss, optimizer=opt, loss_weights=[0.5]) #binary_focal_dice #metrics toevoegen as metrics = []\n","  \n","\n","\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoE-ZOOvZion"},"source":["\n","# # define an encoder block\n","# def define_encoder_block(layer_in, n_filters, batchnorm=True):\n","# \t# weight initialization\n","# \tinit = RandomNormal(stddev=0.02)\n","# \t# add downsampling layer\n","# \tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","# \t# conditionally add batch normalization\n","# \tif batchnorm:\n","# \t\tg = BatchNormalization()(g, training=True)\n","# \t# leaky relu activation\n","# \tg = LeakyReLU(alpha=0.2)(g)\n","# \treturn g\n","\n","# # define a decoder block\n","# def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n","# \t# weight initialization\n","# \tinit = RandomNormal(stddev=0.02)\n","# \t# add upsampling layer\n","# \tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","# \t# add batch normalization\n","# \tg = BatchNormalization()(g, training=True)\n","# \t# conditionally add dropout\n","# \tif dropout:\n","# \t\tg = Dropout(0.5)(g, training=True)\n","# \t# merge with skip connection\n","# \tg = Concatenate()([g, skip_in])\n","# \t# relu activation\n","# \tg = Activation('relu')(g)\n","# \treturn g\n","\n","# # define the standalone generator model\n","# def define_generator(image_shape=(256,256,3)):\n","# \t# weight initialization\n","# \tinit = RandomNormal(stddev=0.02)\n","# \t# image input\n","# \tin_image = Input(shape=image_shape)\n","# \t# encoder model\n","# \te1 = define_encoder_block(in_image, 64, batchnorm=False)\n","# \te2 = define_encoder_block(e1, 128)\n","# \te3 = define_encoder_block(e2, 256)\n","# \te4 = define_encoder_block(e3, 512)\n","# \te5 = define_encoder_block(e4, 512)\n","# \te6 = define_encoder_block(e5, 512)\n","# \te7 = define_encoder_block(e6, 512)\n","# \t# bottleneck, no batch norm and relu\n","# \tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n","# \tb = Activation('relu')(b)\n","# \t# decoder model\n","# \td1 = decoder_block(b, e7, 512)\n","# \td2 = decoder_block(d1, e6, 512)\n","# \td3 = decoder_block(d2, e5, 512)\n","# \td4 = decoder_block(d3, e4, 512, dropout=False)\n","# \td5 = decoder_block(d4, e3, 256, dropout=False)\n","# \td6 = decoder_block(d5, e2, 128, dropout=False)\n","# \td7 = decoder_block(d6, e1, 64, dropout=False)\n","# \t# output\n","# \tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n","# \tout_image = Activation('tanh')(g) #change to sigmoid, then will be between 0 and 1 perfect for binary. \n","# \t# define model\n","# \tmodel = Model(in_image, out_image)\n","# \treturn model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avgPqSk1fTrV"},"source":["\n","# define an encoder block\n","def define_encoder_block(layer_in, n_filters, batchnorm=True):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# add downsampling layer\n","\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","\t# conditionally add batch normalization\n","\tif batchnorm:\n","\t\tg = BatchNormalization()(g, training=True)\n","\t# leaky relu activation\n","\tg = LeakyReLU(alpha=0.2)(g)\n","\treturn g\n","\n","# define a decoder block\n","def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# add upsampling layer\n","\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","\t# add batch normalization\n","\tg = BatchNormalization()(g, training=True)\n","\t# conditionally add dropout\n","\tif dropout:\n","\t\tg = Dropout(0.5)(g, training=True)\n","\t# merge with skip connection\n","\tg = Concatenate()([g, skip_in])\n","\t# relu activation\n","\tg = Activation('relu')(g)\n","\treturn g\n","\n","# define the standalone generator model\n","def define_generator(image_shape=(256,256,3)):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# image input\n","\tin_image = Input(shape=image_shape)\n","\t# encoder model\n","\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n","\te2 = define_encoder_block(e1, 128)\n","\te3 = define_encoder_block(e2, 256)\n","\te4 = define_encoder_block(e3, 512)\n","\te5 = define_encoder_block(e4, 512)\n","\te6 = define_encoder_block(e5, 512)\n","\te7 = define_encoder_block(e6, 512)\n","\t# bottleneck, no batch norm and relu\n","\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n","\tb = Activation('relu')(b)\n","\t# decoder model\n","\td1 = decoder_block(b, e7, 512)\n","\td2 = decoder_block(d1, e6, 512)\n","\td3 = decoder_block(d2, e5, 512)\n","\td4 = decoder_block(d3, e4, 512, dropout=False)\n","\td5 = decoder_block(d4, e3, 256, dropout=False)\n","\td6 = decoder_block(d5, e2, 128, dropout=False)\n","\td7 = decoder_block(d6, e1, 64, dropout=False)\n","\t# output, 1 band without dummy\n","\tg = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n","\tout_image = Activation('tanh')(g) #change to sigmoid, then will be between 0 and 1 perfect for binary. But pix2pix says tanh\n","\t# define model\n","\tmodel = Model(in_image, out_image)\n","\treturn model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dvm2t-rc__lc"},"source":["\n","# # define the combined generator and discriminator model, for updating the generator\n","# def define_gan(g_model, d_model, image_shape):\n","# \t# make weights in the discriminator not trainable\n","# \tfor layer in d_model.layers:\n","# \t\tif not isinstance(layer, BatchNormalization):\n","# \t\t\tlayer.trainable = False\n","# \t# define the source image\n","# \tin_src = Input(shape=image_shape)\n","# \t# connect the source image to the generator input\n","# \tgen_out = g_model(in_src)\n","# \t# connect the source input and generator output to the discriminator input\n","# \tdis_out = d_model([in_src, gen_out])\n","# \t# src image as input, generated image and classification output\n","# \tmodel = Model(in_src, [dis_out, gen_out])\n","# \t# compile model\n","# \topt = Adam(learning_rate=0.0002, beta_1=0.5)\n","#   # opt = Adam(lr=0.0002, beta_1=0.5)\n","# \tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100]) #binary_focal_dice\n","# \treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIwgKZH_pJuC"},"source":["\n","# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model, image_shape):\n","\t# make weights in the discriminator not trainable\n","\tfor layer in d_model.layers:\n","\t\tif not isinstance(layer, BatchNormalization):\n","\t\t\tlayer.trainable = False\n","\t# define the source image\n","\tin_src = Input(shape=image_shape)\n","\t# connect the source image to the generator input\n","\tgen_out = g_model(in_src)\n","\t# connect the source input and generator output to the discriminator input\n","\tdis_out = d_model([in_src, gen_out])\n","\t# src image as input, generated image and classification output\n","\tmodel = Model(in_src, [dis_out, gen_out])\n","\t# compile model\n","\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n","  # opt = Adam(lr=0.0002, beta_1=0.5)\n","\tmodel.compile(loss=[bce_dice_loss, 'mae'], optimizer=opt, loss_weights=[1,100]) #binary_focal_dice\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HlJjFmdufXDX"},"source":["## Load performance / Save model"]},{"cell_type":"code","metadata":{"id":"CusFx_KcB9Yb"},"source":["# def save_model(file_path_to_save,model,step):\n","#   filename = file_path_to_save + 'modelopt__%s.h5' % (step+1)\n","#   model.save(filename,include_optimizer = True) \n","#   print('>Saved model: %s ' % (filename))\n","\n","\n","def save_model(file_path_to_save,model_name,model,step):\n","  filename = file_path_to_save + model_name + '__%s.h5' % (step+1)\n","  model.save(filename,include_optimizer = True) \n","  print('>Saved model: %s ' % (filename))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmyoOvijS5kZ"},"source":["Do a iou per epoch, and plot some specific images"]},{"cell_type":"code","metadata":{"id":"VfSpIzz7uanZ"},"source":["def iou_coe(output, target, axis,threshold=0.0, smooth=1e-5):\n","# def iou_coe(output, target, threshold=0.0, axis=(1,2,3), smooth=1e-5):\n","    \"\"\"\" With tanh threshold = 0.0\n","    \"\"\" \n","\n","    \"\"\"Non-differentiable Intersection over Union (IoU) for comparing the\n","    similarity of two batch of data, usually be used for evaluating binary image segmentation.\n","    The coefficient between 0 to 1, and 1 means totally match.\n","\n","    Parameters\n","    -----------\n","    output : tensor\n","        A batch of distribution with shape: [batch_size, ....], (any dimensions).\n","    target : tensor\n","        The target distribution, format the same with `output`.\n","    threshold : float\n","        The threshold value to be true.\n","    axis : tuple of integer\n","        All dimensions are reduced, default ``(1,2,3)``.\n","    smooth : float\n","        This small value will be added to the numerator and denominator, see ``dice_coe``.\n","\n","    Notes\n","    ------\n","    - IoU cannot be used as training loss, people usually use dice coefficient for training, IoU and hard-dice for evaluating.\n","\n","    \"\"\"\n","    pre = tf.cast(output > threshold, dtype=tf.float32)\n","    truth = tf.cast(target > threshold, dtype=tf.float32)\n","    inse = tf.reduce_sum(tf.multiply(pre, truth), axis=axis)  # AND\n","    union = tf.reduce_sum(tf.cast(tf.add(pre, truth) >= 1, dtype=tf.float32), axis=axis)  # OR\n","    # old axis=[0,1,2,3]\n","    # epsilon = 1e-5\n","    # batch_iou = inse / (union + epsilon)\n","    # new haodong\n","    batch_iou = (inse + smooth) / (union + smooth)\n","    iou = tf.reduce_mean(batch_iou, name='iou_coe')\n","    return iou  # , pre, truth, inse, union"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qodGXRt8T-wY"},"source":["def dice_hard_coe(output, target,  axis, threshold=0.0,smooth=1e-5):\n","# def dice_hard_coe(output, target, threshold=0.0, threshold_to_target = 0.0, axis=(1, 2, 3), smooth=1e-5):\n","\n","    \"\"\"\" With tanh threshold = 0.0\n","    \"\"\" \n","\n","\n","    \"\"\"Non-differentiable Sørensen–Dice coefficient for comparing the similarity\n","    of two batch of data, usually be used for binary image segmentation i.e. labels are binary.\n","    The coefficient between 0 to 1, 1 if totally match.\n","\n","    Parameters\n","    -----------\n","    output : tensor\n","        A distribution with shape: [batch_size, ....], (any dimensions).\n","    target : tensor\n","        The target distribution, format the same with `output`.\n","    threshold : float\n","        The threshold value to be true.\n","    axis : tuple of integer\n","        All dimensions are reduced, default ``(1,2,3)``.\n","    smooth : float\n","        This small value will be added to the numerator and denominator, see ``dice_coe``.\n","\n","    References\n","    -----------\n","    - `Wiki-Dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`__\n","\n","    \"\"\"\n","\n","\n","\n","    output = tf.cast(output > threshold, dtype=tf.float32)\n","    target = tf.cast(target > threshold, dtype=tf.float32) \n","    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)\n","    l = tf.reduce_sum(output, axis=axis)\n","    r = tf.reduce_sum(target, axis=axis)\n","    # old axis=[0,1,2,3]\n","    # hard_dice = 2 * (inse) / (l + r)\n","    # epsilon = 1e-5\n","    # hard_dice = tf.clip_by_value(hard_dice, 0, 1.0-epsilon)\n","    # new haodong\n","    hard_dice = (2. * inse + smooth) / (l + r + smooth)\n","    ##\n","    hard_dice = tf.reduce_mean(hard_dice, name='hard_dice')\n","    return hard_dice\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2b8xjccbSfDD"},"source":["## Defs of load_samples"]},{"cell_type":"code","metadata":{"id":"bLHaXGk6Jyum"},"source":["\n","# select a batch of random samples, returns images and target\n","def generate_real_samples(batch):\n","  # unpack dataset\n","  X1,X2 = batch\n","  n_samples = X1.shape[0] \n","\t# generate 'real' class labels (1)\n","  #patch_shape is a constant\n","  y = ones((n_samples, patch_shape, patch_shape, 1))\n","  return [X1, X2], y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xoQD6AwJz4R"},"source":["# generate a batch of images, returns images and targets\n","def generate_fake_samples(g_model, samples):\n","\t# generate fake instance\n","\tX = g_model.predict(samples)\n","\t# create 'fake' class labels (0)\n","\ty = zeros((len(X), patch_shape, patch_shape, 1))\n","\treturn X, y\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxO0Um-1aRuk"},"source":["## Run GAN model\n","Here need to define that you use TPU\n"]},{"cell_type":"code","metadata":{"id":"wzqyd-_8SsxL"},"source":["\n","image_shape = (256,256,3)\n","image_shape_target = (256,256,1) #needs to be 256,256,1 not 256,256!\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwstExFrJN39"},"source":["with tpu_strategy.scope():\n","  d_model = define_discriminator(image_shape,image_shape_target)\n","  g_model = define_generator(image_shape)\n","  gan_model = define_gan(g_model,d_model,image_shape) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBC4HB-Hbal-"},"source":["patch_shape = d_model.output_shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTOnq4r66kBx"},"source":["# gan_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbitBNub6hxU"},"source":["# d_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvShBjFN6Wht"},"source":["# g_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0GDe6O-vOgA"},"source":["# TF dataset"]},{"cell_type":"markdown","metadata":{"id":"-HK0MyhSxRQ3"},"source":["https://www.kaggle.com/docs/tpu\n","Could add some smart things from here"]},{"cell_type":"markdown","metadata":{"id":"WWwkL2qlXcVd"},"source":["## Parse and decode tfrec"]},{"cell_type":"code","metadata":{"id":"cliWhB1oEwhI"},"source":["#with image as string\n","\n","def parse_record(record):\n","  tfrecord_format = {\n","    \"Y\": tf.io.FixedLenFeature([],tf.string),\n","    \"sar1\": tf.io.FixedLenFeature([],tf.string),\n","    \"sar2\": tf.io.FixedLenFeature([],tf.string),\n","    \"dem\": tf.io.FixedLenFeature([],tf.string)\n","   \n","    }\n","  return tf.io.parse_single_example(record, tfrecord_format) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3FAXFnA1ll2"},"source":["dim = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha4CpsIlHXoP"},"source":["#return to image, so decode parse record\n","def decode_record(record):\n","\n","  record_parsed = parse_record(record)\n","  # dataType = float32\n","  b1 = tf.io.decode_raw(\n","      record_parsed['sar1'],out_type = float\n","  )\n","  b2 = tf.io.decode_raw(\n","      record_parsed['sar2'],out_type = float\n","  )\n","  dem = tf.io.decode_raw(\n","      record_parsed['dem'],out_type = float\n","  )\n","  y = tf.io.decode_raw(\n","      record_parsed['Y'],out_type = float      \n","  )\n","\n","# sometimes values slightly below -1 are in the dem band, filter them to -1. (these are dem values which where below 0 before scaling apparently) \n","  # value = tf.constant(-1.0,dtype=float)\n","  dem = tf.where([dem < -1.000], x=[-1.000], y = [dem])#doesnt work?\n","  # tf.where(dem < value, x=value, y = dem)\n","\n","  # #images are flattened, so have to put back to og shape\n","  b1 = tf.reshape(b1,(dim,dim))  #w, h 256,256\n","  b2 = tf.reshape(b2,(dim,dim))\n","  dem = tf.reshape(dem,(dim,dim))\n","  y1 = tf.reshape(y,(dim,dim,1)) #hopefully y1 is then (256,256,1)\n","\n","  # #might want to stack them before\n","  image_x = tf.stack((b1,b2,dem),axis = -1) #if axis = -1 then get (nr im,w,h,3)\n","\n","  # y2 = tf.experimental.numpy.full_like(y1,-1,dtype= tf.float32)\n","  # y3 = tf.experimental.numpy.full_like(y1,-1,dtype= tf.float32)\n","  # # y2 = tf.experimental.numpy.full_like(y1,0,dtype= tf.float32)\n","  # # y3 = tf.experimental.numpy.full_like(y1,0,dtype= tf.float32)\n","  # image_y = tf.stack((y1,y2,y3),axis = -1)\n","\n","  # return image_x,image_y\n","  return image_x,y1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsYPff9FaCw8"},"source":["## Add augmentations"]},{"cell_type":"code","metadata":{"id":"-0yT8n-oaFF9"},"source":["def dropout(image, size=256, p = 0.75, ct = 12, sz = 0.05):\n","    # input - one image of size [size, size, 3] not a batch of [b, size, size, 3]\n","    # output - image with ct squares of side size sz*size removed\n","    p = tf.cast(tf.random.uniform([],0,1) < p, tf.int32)\n","    if (p == 0)|(ct == 0)|(sz == 0): return image\n","\n","    for k in range(ct):\n","        # choose random location\n","        x = tf.cast(tf.random.uniform([], 0, size),tf.int32)\n","        y = tf.cast(tf.random.uniform([], 0, size),tf.int32)\n","        \n","        # compute square\n","        width = tf.cast(sz*size, tf.int32)*p\n","        ya = tf.math.maximum(0, y-width//2)\n","        yb = tf.math.minimum(size, y+width//2)\n","        xa = tf.math.maximum(0, x-width//2)\n","        xb = tf.math.minimum(size, x+width//2)\n","        \n","        # dropout image\n","        one = image[ya:yb, 0:xa, :]\n","        two = tf.zeros([yb-ya, xb-xa, image.shape[-1]]) \n","        three = image[ya:yb, xb:size, :]\n","        middle = tf.concat([one, two, three], axis=1)\n","        image = tf.concat([image[0:ya, :, :], middle, image[yb:size, :, :]], axis=0)\n","\n","    return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLxgUANfbMkQ"},"source":["\n","def augment(image, mask, seed=P['SEED'],size=P['SIZE']): #not sure if input image,mask is okinstead of record..\n","    # Flip\n","    if tf.random.uniform([], seed=seed) > 0.5:\n","        image = tf.image.flip_left_right(image)\n","        mask = tf.image.flip_left_right(mask)\n","\n","    if tf.random.uniform([],seed=seed) > 0.5:\n","        image = tf.image.flip_up_down(image)\n","        mask = tf.image.flip_up_down(mask)\n","        \n","    if tf.random.uniform([],seed=seed) > .75:\n","        image = tf.image.transpose(image)\n","        mask = tf.image.transpose(mask)\n","      \n","    p_rotate = tf.random.uniform([],seed=seed)\n","    if p_rotate > .75:\n","        image = tf.image.rot90(image, k=3) # rotate 270º\n","        mask = tf.image.rot90(mask, k=3)\n","    elif p_rotate > .5:\n","        image = tf.image.rot90(image, k=2) # rotate 180º\n","        mask = tf.image.rot90(mask, k=2)\n","    elif p_rotate > .25:\n","        image = tf.image.rot90(image, k=1) # rotate 90º\n","        mask = tf.image.rot90(mask, k=1)\n","        \n","    # Drop out. Only for the image with 3 bands\n","    # image = dropout(image, size, p=1.0, ct=10, sz=0.1)\n","                \n","    \n","    \n","    return image, mask\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSWHwINs1z9u"},"source":["## Create data set from tfrec"]},{"cell_type":"code","metadata":{"id":"vFqvSRdZbhNP"},"source":["AUTO = tf.data.AUTOTUNE\n","ignore_order = tf.data.Options()\n","ignore_order.experimental_deterministic = False\n","\n","def get_training_dataset(files, len_shuffle,seed=P['SEED'], batch_size=n_batch):\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    dataset = dataset.with_options(ignore_order)\n","    dataset = dataset.cache()\n","    dataset = dataset.repeat(2)\n","    dataset = dataset.map(lambda ex: decode_record(ex), num_parallel_calls=AUTO)\n","    dataset = dataset.map(augment, num_parallel_calls=AUTO) #maybe should do lambda x y here..\n","    dataset = dataset.shuffle(buffer_size= len_shuffle, seed=seed) #when shuffle??\n","    dataset = dataset.batch(batch_size, drop_remainder=False) #bc seconde one is only 16 for bsize 512.\n","    dataset = dataset.prefetch(AUTO)\n","    \n","    return dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZIL-xg_csH0K"},"source":["def count_examples(filenames):\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(str(filename)).group(1)) for filename in filenames]\n","    \n","    return np.sum(n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWsJesJ9oL6Q"},"source":["##somehow this function doesnt work.. maybe bc drop remainder!!!! --> bc val ls < batch size \n","def get_validation_dataset(files, len_shuffle,ordered=True,seed=P['SEED'], batch_size=n_batch):\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    dataset = dataset.with_options(ignore_order)\n","    dataset = dataset.cache()\n","    # dataset = dataset.map(lambda ex: decode_record(ex), num_parallel_calls=AUTO)\n","    dataset = dataset.map(decode_record, num_parallel_calls=AUTO)\n","    dataset = dataset.shuffle(buffer_size = len_shuffle, seed=seed) \n","    dataset = dataset.batch(batch_size, drop_remainder=False)\n","    dataset = dataset.prefetch(AUTO)\n","\n","    return dataset\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdbGCQw_KU9L"},"source":["## get Kaggle gs, create ds"]},{"cell_type":"markdown","metadata":{"id":"qWOH-_par1p7"},"source":["for training set"]},{"cell_type":"code","metadata":{"id":"1Zoh5bHkjnJ4"},"source":["# train_dataset = get_training_dataset(tfrec_files_all_ls,700)\n","# train_dataset = get_training_dataset(tfrec_files_all_ls_des,450)\n","# train_dataset = get_training_dataset(tfrec_files_ls_asc,len_files_to_shuffle)\n","# train_dataset = get_training_dataset(tfrec_files_ls_des,200)\n","train_dataset = get_training_dataset(tfrec_files_ls,400)\n","# tfrec_files_ls_des"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pc5p6yQNrQXO"},"source":["val_dataset = get_validation_dataset(tfrec_files_val_ls,len_files_val_to_shuffle) \n","# val_dataset = get_validation_dataset(files = tfrec_files_val_ls_asc,len_shuffle = 200) #what buffer size??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grzciea8fRFO"},"source":["### Check if correct"]},{"cell_type":"code","metadata":{"id":"uV8Ojsa_pV0q"},"source":["#dropout in every image?? Lets try without first\n","def check_y(ds):\n","  for [x1,x2] in ds.take(1):\n","    tar = x2[:,:,:,0]\n","    b1 = x1[:,:,:,0]\n","    b2 = x1[:,:,:,1]\n","    dem = x1[:,:,:,2]\n","    print(tar.shape[0])\n","    # for i in range(tar.shape[0]):\n","    for i in range(10):\n","      plt.imshow(b1[i])\n","      plt.axis('off')\n","      plt.show()\n","      plt.close()\n","      plt.imshow(b2[i])\n","      plt.axis('off')\n","      plt.show()\n","      plt.close()\n","      plt.imshow(dem[i])\n","      plt.axis('off')\n","      plt.show()\n","      plt.close()\n","      plt.imshow(tar[i])\n","      plt.axis('off')\n","      plt.show()\n","      plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agtzG29CZA98"},"source":["check_y(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01mSxi2b0D8y"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"yPWbTv0q4t_k"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"cW_KLvcaWIIO"},"source":["Maybe add some stuff to stop training if model is shit -->\n","- if iou and dice of test are still nan after >10 epochs\n","- if loss of dis. is still 0/close to 0 after>10 epochs (gan converge failure)\n"]},{"cell_type":"markdown","metadata":{"id":"XecLsuRxCOZl"},"source":["https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/\n","\n","The loss is reported each training iteration, including the discriminator loss on real examples (d1), discriminator loss on generated or fake examples (d2), and generator loss, which is a weighted average of adversarial and L1 loss (g).\n","\n","\n","This is important. A stable GAN will have a discriminator loss around 0.5, typically between 0.5 and maybe as high as 0.7 or 0.8. The generator loss is typically higher and may hover around 1.0, 1.5, 2.0, or even higher\n","\n","This is an example of the normal or expected loss during training. Namely, discriminator loss for real and fake samples is about the same at or around 0.5, and loss for the generator is slightly higher between 0.5 and 2.0\n","\n","So quit training after x epochs if d1 and/or d2 loss is close to 0. Also if iou remains nan.."]},{"cell_type":"markdown","metadata":{"id":"mrgO_q6beGnw"},"source":["Acutally its possible to directly do iou over batch instead of in for loop I think..."]},{"cell_type":"code","metadata":{"id":"ZevmeGRAoG4Q"},"source":["path_to_save"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZwG54FuoIhl"},"source":["path_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZ1eRiF6Zi0f"},"source":["def train(batch_train,batch_test,d_model=d_model, g_model=g_model, gan_model=gan_model,num_epochs = 100,epoch_stop =30,epoch_save=[49,59,69,79,89,99] ,file_path_to_save_model=path_to_save,file_path_to_save_result=path_results):\n","\n","  \"\"\"\" train model\n","  Here also loss is calculated\n","  Also stops after dloss remains low and/or iou remains nan (aka 0/x ? what gives a nan value? )\n","  Here only the iou and dice for the positive images is calculated\n","  batch_train: the train dataset, tfrecord dataset which is batched\n","  batch_test: the test dataset, tfrecord dataset which is batched, only ls images\n","  No if condition on validation set since the validation is only ls images\n","  \"\"\"\"\"\n","  \n","  #dice and iou over epochs for train and test set.\n","  iou_over_epochs,iou_test_over_epochs = list(), list()\n","  d1_hist, d2_hist, g_hist = list(), list(), list()\n","\n","  for epoch in tqdm(range(num_epochs)):\n","\n","    #metric per image over all image in one epoch\n","    iou_over_epoch,iou_test_over_epoch = list(), list()\n","    d1_epoch, d2_epoch, g_epoch = list(),list(),list()\n","\n","    for [x1,x2] in tqdm(batch_train):\n","\n","      # select a batch of real samples\n","      [src, target], y_real = generate_real_samples([x1,x2]) \n","      # generate a batch of fake samples\n","      gen_target, y_fake = generate_fake_samples(g_model, src)  \n","      # update discriminator for real samples\n","      d_loss1 = d_model.train_on_batch([src,target], y_real)\n","       # update discriminator for generated samples\n","      d_loss2 = d_model.train_on_batch([src, gen_target], y_fake)\n","      # update the generator\n","      g_loss, _, _ = gan_model.train_on_batch(src, [y_real, target])\n","\n","      \n","      #get 0 band!\n","      iou_over_batch = iou_coe(gen_target[:,:,:,0],target[:,:,:,0],axis=(1,2))\n","\n","      #record loss history, to get a plot of converging (hopefully) dis & gen loss over epochs (only for train ds(?))\n","      d1_epoch.append(d_loss1)\n","      d2_epoch.append(d_loss2)\n","      g_epoch.append(g_loss)\n","  \n","      \n","      #save iou\n","      iou_over_epoch.append(iou_over_batch)\n","\n","\n","#retracing might be bc the batch_test has other size bc its smaller, so dif batch sizes..\n","    for [x1,x2] in tqdm(batch_test):\n","      target_pred = g_model.predict(x1)\n","      iou_test = iou_coe(target_pred[:,:,:,0],x2[:,:,:,0],axis=(1,2))\n","      #save iou\n","      iou_test_over_epoch.append(iou_test)\n","\n","\n","    #  #save the metric accross epochs.\n","    iou_test_over_epochs.append(tf.reduce_mean(iou_test_over_epoch))\n","    #save the metric accross epochs, for train\n","    iou_over_epochs.append(tf.reduce_mean(iou_over_epoch))\n","         \n","\n","    # out of batch loop, in epoch loop\n","    # record history over epochs --> could either do mean of each epoch or just leave them for each batch.\n","    d1_hist.append(d1_epoch)\n","    d2_hist.append(d2_epoch)\n","    g_hist.append(g_epoch)\n","\n","    \n","    if tf.equal(epoch,epoch_stop): #this works!\n","      print('d loss at epoch stop',d_loss1)\n","      # print(d_loss1 == np.nan)\n","      # print(d_loss1 == float('nan'))\n","\n","      if tf.math.less(d_loss1,tf.constant(0.01)):\n","        print('then should quit since d1 loss is close to zero:',d_loss1) #works\n","        # break\n","      elif np.isnan(d_loss1):\n","        print(\"its nan\")\n","        # break\n","  \n","\n","    # if tf.math.greater_equal(epoch,epoch_save): #if want to retrain might do it at every tenth model starting at 50..\n","      #if want to retrain (e.g. further tha epoch 100 such as 200, than need to save all three models)\n","    if epoch in epoch_save:\n","      # Better save both as GAN is the orchestrated work of both the generator and discriminator together. --> seems like only gan?\n","      save_model(file_path_to_save_model,'g_model',g_model,epoch)\n","      save_model(file_path_to_save_model,'d_model',d_model,epoch)\n","      save_model(file_path_to_save_model,'gan_model',gan_model,epoch)\n","\n","\n","  #not sure if this would be too much for memory..\n","  #save losses\n","  fn_d1 = file_path_to_save_result + 'd1.txt'\n","  np.savetxt(fn_d1, d1_hist) #these are nested lists!\n","  fn_d2 = file_path_to_save_result + 'd2.txt'\n","  np.savetxt(fn_d2, d2_hist)\n","  fn_g = file_path_to_save_result + 'g.txt'\n","  np.savetxt(fn_g, g_hist)\n","\n","\n","  #save dice and iou as well\n","  fn_iou = file_path_to_save_result+'iou_over_epochs.txt'\n","  np.savetxt(fn_iou, iou_over_epochs)\n","\n","  fn_iou = file_path_to_save_result+'iou_test_over_epochs.txt' \n","  np.savetxt(fn_iou, iou_test_over_epochs)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KChx80x8BUA9"},"source":[""]},{"cell_type":"code","metadata":{"id":"__MAaavM0T0O"},"source":["train(train_dataset,val_dataset)  #3:18 28% for bsize 8 with repeat 2 shit.... Maybe gpu is faster than tpy if you only use bsize 8?\n","#65 percent, 7:41h\n","# 132 batches per epoch, sure I didnt accidentally put repeat(3)?\n","#70 8:17h\n","#quit at 72 percent at 8:31 :((((("],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LFiczKYQKiej"},"source":["# Retrain loaded model"]},{"cell_type":"code","metadata":{"id":"eQKBy6LSKlao"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9bbWpUHKmqw"},"source":["## load model"]},{"cell_type":"code","metadata":{"id":"tSAVljfNKnub"},"source":["# example of loading a pix2pix model and using it for image to image translation\n","from keras.models import load_model\n","from numpy import load\n","from numpy import vstack\n","from numpy.random import randint\n","\n","!ls -la {path_to_save}\n","\n","fn_models = [str(f) for f in pathlib.Path(path_to_save).glob('*.h5')]\n","fn_models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"anl5jS_BKqbw"},"source":["# fn_d_model = path_to_save + \"d_model__100.h5\"\n","# fn_g_model = path_to_save + \"g_model__100.h5\"\n","fn_gan_model = fn_models[-1]\n","fn_d_model = fn_models[-2]\n","fn_g_model = fn_models[-3]\n","\n","print(\"the models are\",fn_gan_model,fn_d_model,fn_g_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A95eouVrKu1P"},"source":["\n","\n","epoch_loaded_model = int(fn_gan_model.split(\"__\",1)[-1].split(\".h5\")[0]) #but then al should have names with __\n","epoch_loaded_model\n","\n","# But then should have the gan, gen and dis...\n","\n","with tpu_strategy.scope():\n","    # load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n","    d_model = tf.keras.models.load_model(fn_d_model)\n","    g_model = tf.keras.models.load_model(fn_g_model)\n","    gan_model = tf.keras.models.load_model(fn_gan_model)#,custom_objects = {'binary_focal_loss_plus_dice_loss':binary_focal_dice_loss}, options=load_locally) # loading in Tensorflow's \"SavedModel\" format\n","\n","d_model.summary()\n","\n","g_model.summary()\n","\n","gan_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3evtGRAQK8Vj"},"source":["## Run train on reloaded model\n","\n","new path to save results and model to..\n","add real epoch nr for use for file name f saved model. ad possibly in results name\n"]},{"cell_type":"code","metadata":{"id":"1Em6DBGCK5Te"},"source":["\n","def train(batch_train,batch_test,d_model=d_model, g_model=g_model, gan_model=gan_model,num_epochs = 80,epoch_stop =30,epoch_begin_rerun = epoch_loaded_model,epoch_save=[49,59,69,79,89,99] ,file_path_to_save_model=path_to_save,file_path_to_save_result=path_results):\n","\n","  \"\"\"\" train model\n","  Here also loss is calculated\n","  Also stops after dloss remains low and/or iou remains nan (aka 0/x ? what gives a nan value? )\n","  Here only the iou and dice for the positive images is calculated\n","  batch_train: the train dataset, tfrecord dataset which is batched\n","  batch_test: the test dataset, tfrecord dataset which is batched, only ls images\n","  No if condition on validation set since the validation is only ls images\n","  \"\"\"\"\"\n","\n","  file_path_to_save_model = file_path_to_save_model + \"rerun1/\"\n","  file_path_to_save_result = file_path_to_save_result + \"rerun1/\"\n","  \n","  #dice and iou over epochs for train and test set.\n","  iou_over_epochs,iou_test_over_epochs = list(), list()\n","  d1_hist, d2_hist, g_hist = list(), list(), list()\n","\n","  for epoch in tqdm(range(num_epochs)):\n","\n","    #metric per image over all image in one epoch\n","    iou_over_epoch,iou_test_over_epoch = list(), list()\n","    d1_epoch, d2_epoch, g_epoch = list(),list(),list()\n","\n","    for [x1,x2] in tqdm(batch_train):\n","\n","      # select a batch of real samples\n","      [src, target], y_real = generate_real_samples([x1,x2]) \n","      # generate a batch of fake samples\n","      gen_target, y_fake = generate_fake_samples(g_model, src)  \n","      # update discriminator for real samples\n","      d_loss1 = d_model.train_on_batch([src,target], y_real)\n","       # update discriminator for generated samples\n","      d_loss2 = d_model.train_on_batch([src, gen_target], y_fake)\n","      # update the generator\n","      g_loss, _, _ = gan_model.train_on_batch(src, [y_real, target])\n","\n","\n","      #record loss history, to get a plot of converging (hopefully) dis & gen loss over epochs (only for train ds(?))\n","      d1_epoch.append(d_loss1)\n","      d2_epoch.append(d_loss2)\n","      g_epoch.append(g_loss)\n","    \n","\n","      #iou with condition\n","      iou_over_batch = iou_coe(gen_target[:,:,:,0],target[:,:,:,0],axis=(1,2))\n","    \n","      #save iou\n","      iou_over_epoch.append(iou_over_batch)\n","  \n","#retracing might be bc the batch_test has other size bc its smaller, so dif batch sizes..\n","    for [x1,x2] in tqdm(batch_test):\n","      target_pred = g_model.predict(x1)\n","\n","      iou_over_batch = iou_coe(target_pred[:,:,:,0],x2[:,:,:,0],axis=(1,2))\n","       \n","      #save iou\n","      iou_test_over_epoch.append(iou_over_batch)\n","\n","\n","    #  #save the metric accross epochs.\n","    iou_test_over_epochs.append(tf.reduce_mean(iou_test_over_epoch))\n","    iou_over_epochs.append(tf.reduce_mean(iou_over_epoch))\n","    \n","         \n","\n","    # out of batch loop, in epoch loop\n","    # record history over epochs --> could either do mean of each epoch or just leave them for each batch.\n","    d1_hist.append(d1_epoch)\n","    d2_hist.append(d2_epoch)\n","    g_hist.append(g_epoch)\n","\n","  \n","    if tf.equal(epoch,epoch_stop): #this works!\n","      print('d loss at epoch stop',d_loss1)\n","      # print(d_loss1 == np.nan)\n","      # print(d_loss1 == float('nan'))\n","\n","      if tf.math.less(d_loss1,tf.constant(0.01)):\n","        print('then should quit since d1 loss is close to zero:',d_loss1) #works\n","        # break\n","      elif np.isnan(d_loss1):\n","        print(\"its nan\")\n","        # break\n","  \n","\n","    # if tf.math.greater_equal(epoch,epoch_save): #if want to retrain might do it at every tenth model starting at 50..\n","      #if want to retrain (e.g. further tha epoch 100 such as 200, than need to save all three models)\n","    if epoch in epoch_save:\n","      epoch_rerun = epoch + epoch_begin_rerun \n","      # Better save both as GAN is the orchestrated work of both the generator and discriminator together. --> seems like only gan?\n","      save_model(file_path_to_save_model,\"g_model\",g_model,epoch_rerun)\n","      save_model(file_path_to_save_model,\"d_model\",d_model,epoch_rerun)\n","      save_model(file_path_to_save_model,\"gan_model\",gan_model,epoch_rerun)\n","\n","\n","\n","\n","  #not sure if this would be too much for memory..\n","  #save losses\n","  fn_d1 = file_path_to_save_result + 'd1.txt'\n","  np.savetxt(fn_d1, d1_hist) #these are nested lists!\n","  fn_d2 = file_path_to_save_result + 'd2.txt'\n","  np.savetxt(fn_d2, d2_hist)\n","  fn_g = file_path_to_save_result + 'g.txt'\n","  np.savetxt(fn_g, g_hist)\n","\n","\n","  #save  iou as well\n","  fn_iou = file_path_to_save_result+'iou_over_epochs.txt'\n","  np.savetxt(fn_iou, iou_over_epochs)\n","\n","  fn_iou = file_path_to_save_result+'iou_test_over_epochs.txt' \n","  np.savetxt(fn_iou, iou_test_over_epochs)\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0qmsPa6K-8G"},"source":["\n","train(train_dataset,val_dataset) "],"execution_count":null,"outputs":[]}]}