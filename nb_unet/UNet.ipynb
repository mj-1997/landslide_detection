{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"UNet.ipynb","provenance":[],"collapsed_sections":["3-s8ZLiK1iGP","G0GDe6O-vOgA"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9LlN8wgNNclE"},"source":["##Start up"]},{"cell_type":"markdown","metadata":{"id":"nG2A_pxD1fXN"},"source":["### GDrive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvA6uejaN4aA","executionInfo":{"elapsed":41747,"status":"ok","timestamp":1634019947830,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"84ef110f-fdc3-4c01-d82e-0321db62da73"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"z2kePzS3uSDb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-s8ZLiK1iGP"},"source":["### Enable TPU\n","Edit --> Notebook Settings --> TPU\n","Then check if TPU works"]},{"cell_type":"markdown","metadata":{"id":"8L_G9P3QHMFM"},"source":["Ensure that the number of instances is perfectly divisible by the steps_per_epoch parameter so that all the instances are used during\n","training. For example, we have 60000 instances in our training set. 60000\n","is divisible by 50 so that means all our instances are fed into the model without any leftovers. If you hit run, it should start training on a TPU instance after a short while:\n","\n","\n","Basically should have that data is 256 all the time in batch not a leftover with less... Could get rid of the data that does not fit into it?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNkSvTzz1tde","executionInfo":{"elapsed":18658,"status":"ok","timestamp":1634019966472,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"4b699193-d02f-414e-d961-f0758a55846b"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Tensorflow version 2.6.0\n","Running on TPU  ['10.72.9.178:8470']\n","INFO:tensorflow:Clearing out eager caches\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.72.9.178:8470\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.72.9.178:8470\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"data":{"text/plain":["<tensorflow.python.tpu.topology.Topology at 0x7f7b7f97d650>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xF0zrSmSfTO-","executionInfo":{"elapsed":706,"status":"ok","timestamp":1634019967149,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"7e27d776-4b59-4016-dd87-ad31093016b0"},"source":["#This strategy ssems to not work?\n","\n","# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"up3WB1dw1xnl"},"source":["### Import lib"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"owP_30jNI9qN","executionInfo":{"elapsed":40629,"status":"ok","timestamp":1634020007770,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"28403a55-46d6-4b99-c73e-3a211f82d5aa"},"source":["pip install tensorflow-transform==1.3.0"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-transform==1.3.0\n","  Downloading tensorflow_transform-1.3.0-py3-none-any.whl (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (1.3.0)\n","Collecting apache-beam[gcp]<3,>=2.31\n","  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 49.5 MB/s \n","\u001b[?25hCollecting pyarrow<3,>=1\n","  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n","\u001b[K     |████████████████████████████████| 17.7 MB 77 kB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-metadata<1.3.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (1.2.0)\n","Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (2.6.0)\n","Requirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (0.12.0)\n","Collecting tfx-bsl<1.4.0,>=1.3.0\n","  Downloading tfx_bsl-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.0 MB)\n","\u001b[K     |████████████████████████████████| 19.0 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (3.17.3)\n","Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform==1.3.0) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<0.13,>=0.9->tensorflow-transform==1.3.0) (1.15.0)\n","Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n","  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 66.5 MB/s \n","\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.7)\n","Collecting fastavro<2,>=0.21.4\n","  Downloading fastavro-1.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 42.7 MB/s \n","\u001b[?25hCollecting orjson<4.0\n","  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n","\u001b[K     |████████████████████████████████| 249 kB 55.7 MB/s \n","\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 744 kB/s \n","\u001b[?25hRequirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (0.17.4)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2.8.2)\n","Collecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (3.12.0)\n","Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (3.7.4.3)\n","Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (4.1.3)\n","Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.41.0)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2018.9)\n","Collecting future<1.0.0,>=0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 58.8 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.0.3)\n","Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (4.2.4)\n","Collecting google-apitools<0.5.32,>=0.5.31\n","  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n","\u001b[K     |████████████████████████████████| 173 kB 45.8 MB/s \n","\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n","  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n","\u001b[K     |████████████████████████████████| 255 kB 44.3 MB/s \n","\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0\n","  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n","\u001b[K     |████████████████████████████████| 169 kB 49.0 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.21.0)\n","Requirement already satisfied: google-auth<2,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.35.0)\n","Collecting google-cloud-videointelligence<2,>=1.8.0\n","  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n","\u001b[K     |████████████████████████████████| 183 kB 54.4 MB/s \n","\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n","  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n","\u001b[K     |████████████████████████████████| 267 kB 30.5 MB/s \n","\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n","  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n","\u001b[K     |████████████████████████████████| 435 kB 58.6 MB/s \n","\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0\n","  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.8.0)\n","Collecting google-cloud-language<2,>=1.3.0\n","  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 1.6 MB/s \n","\u001b[?25hCollecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n","  Downloading google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 56.0 MB/s \n","\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n","  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n","Collecting fasteners>=0.14\n","  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (4.7.2)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (0.2.8)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (0.4.1)\n","Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.26.3)\n","Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n","  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n","Collecting google-cloud-core<2,>=0.28.1\n","  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (21.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.53.0)\n","Collecting proto-plus>=1.15.0\n","  Downloading proto_plus-1.19.5-py3-none-any.whl (44 kB)\n","\u001b[K     |████████████████████████████████| 44 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (0.6.2)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (0.4.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2.4.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (1.24.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2.0.6)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.3.0) (2021.5.30)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.1.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (3.1.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.1.2)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (0.37.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.6.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.12.1)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (2.6.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (3.3.0)\n","Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (5.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (2.6.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (0.4.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (0.2.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.12)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (3.3.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (0.4.6)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (4.8.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (3.1.1)\n","Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15\n","  Downloading tensorflow_serving_api-2.6.0-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.4.0,>=1.3.0->tensorflow-transform==1.3.0) (1.1.5)\n","Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.4.0,>=1.3.0->tensorflow-transform==1.3.0) (1.12.8)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.4.0,>=1.3.0->tensorflow-transform==1.3.0) (0.0.4)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.4.0,>=1.3.0->tensorflow-transform==1.3.0) (3.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tensorflow-transform==1.3.0) (3.6.0)\n","Building wheels for collected packages: avro-python3, dill, future, google-apitools, grpc-google-iam-v1\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=400a36d588bb82958525edae6c853ba83cf5a84b0e88d58a14e1dd3b1a7a31d3\n","  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=a264e49cc6525e59466a6e30884862f1fb59b80f137db19efafaed1afda41a05\n","  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=a1f226f97bcf54a1baa5120a02a05e24a33e60a187e482ba942acf4ecf0fd6b1\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=a294bcdd00bd6e156c7f6e51cb094a669aa1fef97ece2c9110446987d6c4a899\n","  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n","  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=ed9bfa882a2537fcd7a40fcb558968f97af3364ab830702ab4e3f6871e322e34\n","  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n","Successfully built avro-python3 dill future google-apitools grpc-google-iam-v1\n","Installing collected packages: requests, grpcio-gcp, pyarrow, proto-plus, orjson, hdfs, grpc-google-iam-v1, google-cloud-core, future, fasteners, fastavro, dill, avro-python3, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-apitools, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-transform\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 3.0.0\n","    Uninstalling pyarrow-3.0.0:\n","      Successfully uninstalled pyarrow-3.0.0\n","  Attempting uninstall: google-cloud-core\n","    Found existing installation: google-cloud-core 1.0.3\n","    Uninstalling google-cloud-core-1.0.3:\n","      Successfully uninstalled google-cloud-core-1.0.3\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","  Attempting uninstall: google-cloud-language\n","    Found existing installation: google-cloud-language 1.2.0\n","    Uninstalling google-cloud-language-1.2.0:\n","      Successfully uninstalled google-cloud-language-1.2.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed apache-beam-2.33.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.5 fasteners-0.16.3 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 orjson-3.6.4 proto-plus-1.19.5 pyarrow-2.0.0 requests-2.26.0 tensorflow-serving-api-2.6.0 tensorflow-transform-1.3.0 tfx-bsl-1.3.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dill","google","requests"]}}},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"id":"slOVQHLmWkYO"},"source":["\n","import tensorflow_transform as tft\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8v_0K5UScc4v"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VBCw6Pxchlz","executionInfo":{"elapsed":22,"status":"ok","timestamp":1634020008445,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"eb9d5731-5c6d-4e9d-ec00-16e5c0c7b64e"},"source":["print (tf.version)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"]}]},{"cell_type":"code","metadata":{"id":"jmeVxr1mIGmg"},"source":["# pip install --upgrade tensorflow-estimator==2.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zhi-IN5jWgwn","executionInfo":{"elapsed":4082,"status":"ok","timestamp":1634020012512,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"81c99c63-0ff8-46c5-b706-f257dfaa666f"},"source":["#Genna\n","!pip install -q segmentation_models\n","\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40 kB 43.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 5.7 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAgcxoJg4f2T","executionInfo":{"elapsed":638,"status":"ok","timestamp":1634020013136,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"079581ef-23e2-4b1e-d194-f6977b5c1694"},"source":["from segmentation_models.losses import bce_dice_loss"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"]}]},{"cell_type":"code","metadata":{"id":"F_hi1Kbt6Laz"},"source":["# from segmentation_models.losses import dice_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_52JyrAHNhH2"},"source":["import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","# import tensorflow as tf\n","import pathlib\n","import re\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","# import itertools\n","import time\n","# from sklearn.metrics import confusion_matrix\n","# from sklearn.metrics import classification_report\n","# import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9g6c-bTchAic"},"source":["\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","%matplotlib inline\n","\n","from tqdm import tqdm_notebook, tnrange\n","from itertools import chain\n","from skimage.io import imread, imshow, concatenate_images\n","from skimage.transform import resize\n","from skimage.morphology import label\n","from sklearn.model_selection import train_test_split\n","\n","# import tensorflow as tf\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sFlWiKraHpu2"},"source":["from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n","from keras.layers.core import Lambda, RepeatVector, Reshape\n","from keras.layers.convolutional import Conv2D, Conv2DTranspose\n","from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n","from keras.layers.merge import concatenate, add\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","# from keras.optimizers import Adam #worked before since 19-08-2021 doesnt work anymore so now import via tf\n","\n","\n","from tensorflow.keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgDrqIr_mn7B"},"source":["from numpy import load\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randint\n","# from keras.optimizers import Adam\n","from keras.initializers import RandomNormal\n","from keras.models import Model\n","from keras.models import Input\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","from keras.layers import Dropout\n","from keras.layers import BatchNormalization\n","from keras.layers import LeakyReLU\n","# from matplotlib import pyplot\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OX8_6AzmyZCg"},"source":["## Choose properties"]},{"cell_type":"code","metadata":{"id":"9xpxFYkXjSlR"},"source":["# global parameters\n","P = {\n","\n","    'SEED': 42,\n","     \n","    # image size to sample\n","    # 'SIZE': 160, \n","    # 'SIZE': 224, \n","     'SIZE': 256, \n","    # 'SIZE': 320,\n","    # 'BATCH_SIZE': 32 * strategy.num_replicas_in_sync, #256.. 64 is 512. \n","    \n","    # 'FOLDS': 5,\n","    \n","    # 'LEARNING_RATE': 0.00075,\n","    \n","    'EPOCHS': 100,\n","     \n","    # 'VERSION': 'v6'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuohlkp76i0j"},"source":["# BATCH_SIZE_PER_REPLICA = 64\n","# GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","# tpu_strategy.num_replicas_in_sync #8.\n","# 256/8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLP1lV2ShDWU"},"source":["n_batch = 512\n","num_epochs = 100 #100 #200 in og pix2pix recommended\n","# path_base = \"/content/drive/MyDrive/Thesis/Unet_v4/results/dif_incl_mask_v1/\"\n","path_base = \"/content/drive/MyDrive/Thesis/Unet_v4/all_ls/\"\n","\n","# os.makedirs(path_base)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTMswbjEhDWW"},"source":["name = \"attempt_2_1110_asc_and_des_bsize_\" + str(n_batch) \n","path_results = path_base + \"results/\" +  name + \"/\"\n","os.makedirs(path_results)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYXAI028hDWY"},"source":["\n","path_to_save = path_base + \"models/\" + name + \"/\" #hceck if works \n","os.makedirs(path_to_save) #seems as if this is not made..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nALrcFDnZMxA","executionInfo":{"elapsed":361,"status":"ok","timestamp":1634020015397,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"8611fc18-1410-43ec-b8ea-bc16d44af3a7"},"source":["path_learning_curve = path_base + \"learning_curves/\" + name + \"/\"\n","os.makedirs(path_learning_curve)\n","path_learning_curve"],"execution_count":null,"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Thesis/Unet_v4/all_ls/learning_curves/attempt_2_1110_asc_and_des_bsize_512/'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"KHM-Arpm-I3o"},"source":["def remove_files(dir):\n","  for f in os.listdir(dir):\n","      os.remove(os.path.join(dir, f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixs7bTI3-TaS"},"source":["# remove_files(path_to_save)\n","# remove_files(path_results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_s19EohvZpJR","executionInfo":{"elapsed":14,"status":"ok","timestamp":1634020015665,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"1413c26e-8ae0-4024-f938-90a70a5ed52b"},"source":["!ls -la {path_to_save}\n","!ls -la {path_results}"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["total 0\n","total 0\n"]}]},{"cell_type":"markdown","metadata":{"id":"MnasP0Na_kvs"},"source":["files from kaggle"]},{"cell_type":"code","metadata":{"id":"Q-8L8xSarfvM"},"source":["# tfrec_files_train = tf.io.gfile.glob('gs://kds-4a817e787c8a7fee3ad00543d64eebbf33a868ce7af53de4fbb340ef/*')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hR-32_xRbs22"},"source":["# tfrec_files_val= tf.io.gfile.glob('gs://kds-54c443e29c2dd043ed5923b96618f7c925f9492d97976baff14524fc/*')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jm0tFLGfv9x_"},"source":["# tfrec_files_test = tf.io.gfile.glob('gs://kds-fb4c9db2eeda8b5477c4a6f6c0060aff5b6e581e9a031e7936b918cf/*')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07YnnkRINOgH"},"source":["# len_files_to_shuffle = (len(tfrec_files_train)) * 100\n","# len_files_to_shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eACO6VseNOgH"},"source":["# len_files_val_to_shuffle = (len(tfrec_files_val)) * 100\n","# len_files_val_to_shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1sebbhzBQCz"},"source":["# tfrec_files_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdfcRyiVBR39"},"source":["# tfrec_files_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e13rQFBYcXYS","executionInfo":{"elapsed":22,"status":"ok","timestamp":1634020015878,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"20eac26f-13c7-4876-bdec-0054d76ac390"},"source":["tfrec_files_all_ls = tf.io.gfile.glob('gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/*')\n","print('all ls files',tfrec_files_all_ls)\n","tfrec_files_all_ls_des = tfrec_files_all_ls[6:]\n","print('des ls files',tfrec_files_all_ls_des)\n","tfrec_files_all_ls_asc = tfrec_files_all_ls[:6]\n","print('asc ls files',tfrec_files_all_ls_asc)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["all ls files ['gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_0.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_1.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_2.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_3.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_4.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_5.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_0.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_1.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_2.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_3.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_4.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_5.tfrec']\n","des ls files ['gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_0.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_1.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_2.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_3.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_4.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_des_5.tfrec']\n","asc ls files ['gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_0.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_1.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_2.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_3.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_4.tfrec', 'gs://kds-107a311b00ae5f20d08f8918fa9855818659144fe84176374977d289/all_ls_asc_5.tfrec']\n"]}]},{"cell_type":"code","metadata":{"id":"c8YgSnsPdPvU"},"source":["# tfrec_files_big_ls = tf.io.gfile.glob('gs://kds-2fce72c5034cc086f74d9f3987cda7e253d4f468e128d93560b9a47b/*')\n","# # print('all ls files',tfrec_files_big_ls)\n","# tfrec_files_big_ls_des = tfrec_files_big_ls[3:]\n","# print('des ls files',tfrec_files_big_ls_des)\n","# tfrec_files_big_ls_asc = tfrec_files_big_ls[:3]\n","# print('des asc val files',tfrec_files_big_ls_asc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pU0ssHGLdRgx","executionInfo":{"elapsed":229,"status":"ok","timestamp":1634020016090,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"46548d5a-d6b3-484e-c099-d9a01fd8195f"},"source":["tfrec_files_val_ls = tf.io.gfile.glob('gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/*')\n","print('all ls val files',tfrec_files_val_ls)\n","tfrec_files_val_ls_des =  tfrec_files_val_ls[2:]\n","print('des ls val files',tfrec_files_val_ls_des)\n","tfrec_files_val_ls_asc =  tfrec_files_val_ls[:2]\n","print('asc val files',tfrec_files_val_ls_asc)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["all ls val files ['gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_asc_0.tfrec', 'gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_asc_1.tfrec', 'gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_des_0.tfrec', 'gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_des_1.tfrec']\n","des ls val files ['gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_des_0.tfrec', 'gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_des_1.tfrec']\n","asc val files ['gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_asc_0.tfrec', 'gs://kds-30ca5b900fbbba0fa21aab1bf32870ea6f9dc4f8a2aa7edb3b7bebfc/all_ls_val_asc_1.tfrec']\n"]}]},{"cell_type":"code","metadata":{"id":"sCtlfqDcNSbY"},"source":["tfrec_files_train = tfrec_files_all_ls\n","tfrec_files_val = tfrec_files_val_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftnObXjuNpHO","executionInfo":{"elapsed":25,"status":"ok","timestamp":1634020016093,"user":{"displayName":"marja machielse","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00933235277742921282"},"user_tz":-120},"outputId":"c271b981-0113-4ecd-ca07-9cfd44e7c4bf"},"source":["len_files_to_shuffle = (len(tfrec_files_train)) * 100 #the last one has something between one and hundred\n","print('len to shuffle',len_files_to_shuffle)\n","\n","len_files_val_to_shuffle = (len(tfrec_files_val)) * 50 #the last one has something between one and hundred. But here its half since only val des or val asc\n","print('len to shuffle val',len_files_val_to_shuffle)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["len to shuffle 1200\n","len to shuffle val 200\n"]}]},{"cell_type":"markdown","metadata":{"id":"G0GDe6O-vOgA"},"source":["# TF dataset"]},{"cell_type":"markdown","metadata":{"id":"-HK0MyhSxRQ3"},"source":["https://www.kaggle.com/docs/tpu\n","Could add some smart things from here"]},{"cell_type":"markdown","metadata":{"id":"WWwkL2qlXcVd"},"source":["## Parse and decode tfrec"]},{"cell_type":"code","metadata":{"id":"lA5ZE7cKkELq"},"source":["# def scale_to_0_1(image,mask):\n","#     \"\"\"Scale the bands from -1,1 to 0,1 \"\"\"\n","#     x_normalized = tft.scale_to_0_1(image)\n","#     y_normalized = tft.scale_to_0_1(mask)\n","#     return x_normalized,y_normalized\n","\n","def scale_to_0_1(image):\n","    \"\"\"Scale the bands from -1,1 to 0,1 \"\"\"\n","    image_band_normalized = tft.scale_to_0_1(image)\n","    return image_band_normalized\n","\n","\n","#or try image + 1 / 2 yourself. (from -1,1 to 0,1). \n","\n","# def scale_to_0_1(image_band):\n","#    \"\"\"Scale the bands from -1,1 to 0,1\n","#    This gives errors so far, with image shapes mainly \"\"\"\n","#    image_band_add = tf.math.add(image_band,tf.constant([1],dtype = float,shape=(256,256,1))) #256,256 or 256,256,1\n","#    image_band_normalized = tf.math.divide(image_band_add,tf.constant([2],dtype = float,shape=(256,256,1)))\n","#    return image_band_normalized\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cliWhB1oEwhI"},"source":["#uncomment for amslope representation\n","\n","def parse_record(record):\n","  tfrecord_format = {\n","    \"Y\": tf.io.FixedLenFeature([],tf.string),\n","    \"sar1\": tf.io.FixedLenFeature([],tf.string),\n","    \"sar2\": tf.io.FixedLenFeature([],tf.string),\n","    \"dem\": tf.io.FixedLenFeature([],tf.string)\n","   \n","    }\n","  return tf.io.parse_single_example(record, tfrecord_format) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3FAXFnA1ll2"},"source":["dim = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHmwpBh5MJWc"},"source":["#uncomment for ampslope representation\n","def decode_record(record):\n","\n","  record_parsed = parse_record(record)\n","  # dataType = float32\n","  b1 = tf.io.decode_raw(\n","      record_parsed['sar1'],out_type = float\n","  )\n","  b2 = tf.io.decode_raw(\n","      record_parsed['sar2'],out_type = float\n","  )\n","  dem = tf.io.decode_raw(\n","      record_parsed['dem'],out_type = float\n","  )\n","  y = tf.io.decode_raw(\n","      record_parsed['Y'],out_type = float      \n","  )\n","\n","  #first scale to corr -1,1 for dem\n","  dem = tf.where([dem < -1.000], x=[-1.000], y = [dem])\n","  \n","  #maybe scale to 0,1 here\n","  b1_norm = scale_to_0_1(b1)\n","  b2_norm = scale_to_0_1(b2)\n","  dem_norm = scale_to_0_1(dem)\n","  y_norm = scale_to_0_1(y)\n","\n","\n","\n","\n","\n","  # #images are flattened, so have to put back to og shape\n","  b1_rs = tf.reshape(b1_norm,(dim,dim))  #w, h 256,256\n","  b2_rs = tf.reshape(b2_norm,(dim,dim))\n","  dem_rs = tf.reshape(dem_norm,(dim,dim))\n","  y1_rs = tf.reshape(y_norm,(dim,dim,1)) #hopefully y1 is then (256,256,1)\n","\n","  # #might want to stack them before\n","  image_x = tf.stack((b1_rs,b2_rs,dem_rs),axis = -1) #if axis = -1 then get (nr im,w,h,3)\n","\n","  return image_x,y1_rs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"53W5NcyHIINB"},"source":["#uncomment for A_ratio representation\n","\n","# def parse_record(record):\n","#   tfrecord_format = {\n","#     \"x\": tf.io.FixedLenFeature([],tf.string),\n","#     \"Y\": tf.io.FixedLenFeature([],tf.string)\n","#     }\n","#   return tf.io.parse_single_example(record, tfrecord_format) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bL92wCbsIIGS"},"source":["#uncomment for A_ratio representation\n","# def decode_record(record):\n","\n","#   record_parsed = parse_record(record)\n","#   # dataType = float32\n","#   x = tf.io.decode_raw(\n","#       record_parsed['x'],out_type = float\n","#   )\n","#   y = tf.io.decode_raw(\n","#       record_parsed['Y'],out_type = float      \n","#   )\n","\n","\n","#   image_x = tf.reshape(x,(dim,dim,1))\n","#   image_y = tf.reshape(y,(dim,dim,1))\n","\n","\n","\n","#   return image_x,image_y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha4CpsIlHXoP"},"source":["# #return to image, so decode parse record\n","# def decode_record(record):\n","\n","#   record_parsed = parse_record(record)\n","\n","#   x = tf.io.decode_raw(\n","#       record_parsed['x'],out_type = float\n","#   )\n","#   y = tf.io.decode_raw(\n","#       record_parsed['Y'],out_type = float      \n","#   )\n","\n","#   #maybe scale to 0,1 here\n","#   x_norm = scale_to_0_1(x)\n","#   y_norm = scale_to_0_1(y)\n","\n","#   image_x = tf.reshape(x_norm,(dim,dim,1))\n","#   image_y = tf.reshape(y_norm,(dim,dim,1))\n","\n","\n","\n","\n","#   return image_x,image_y\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsYPff9FaCw8"},"source":["## Add augmentations"]},{"cell_type":"code","metadata":{"id":"-0yT8n-oaFF9"},"source":["def dropout(image, size=256, p = 0.75, ct = 12, sz = 0.05):\n","    # input - one image of size [size, size, 3] not a batch of [b, size, size, 3]\n","    # output - image with ct squares of side size sz*size removed\n","    p = tf.cast(tf.random.uniform([],0,1) < p, tf.int32)\n","    if (p == 0)|(ct == 0)|(sz == 0): return image\n","\n","    for k in range(ct):\n","        # choose random location\n","        x = tf.cast(tf.random.uniform([], 0, size),tf.int32)\n","        y = tf.cast(tf.random.uniform([], 0, size),tf.int32)\n","        \n","        # compute square\n","        width = tf.cast(sz*size, tf.int32)*p\n","        ya = tf.math.maximum(0, y-width//2)\n","        yb = tf.math.minimum(size, y+width//2)\n","        xa = tf.math.maximum(0, x-width//2)\n","        xb = tf.math.minimum(size, x+width//2)\n","        \n","        # dropout image\n","        one = image[ya:yb, 0:xa, :]\n","        two = tf.zeros([yb-ya, xb-xa, image.shape[-1]]) \n","        three = image[ya:yb, xb:size, :]\n","        middle = tf.concat([one, two, three], axis=1)\n","        image = tf.concat([image[0:ya, :, :], middle, image[yb:size, :, :]], axis=0)\n","\n","    return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLxgUANfbMkQ"},"source":["\n","def augment(image, mask, seed=P['SEED'],size=P['SIZE']): #not sure if input image,mask is okinstead of record..\n","    # Flip\n","    if tf.random.uniform([], seed=seed) > 0.5:\n","        image = tf.image.flip_left_right(image)\n","        mask = tf.image.flip_left_right(mask)\n","\n","    if tf.random.uniform([],seed=seed) > 0.5:\n","        image = tf.image.flip_up_down(image)\n","        mask = tf.image.flip_up_down(mask)\n","        \n","    if tf.random.uniform([],seed=seed) > .75:\n","        image = tf.image.transpose(image)\n","        mask = tf.image.transpose(mask)\n","      \n","    p_rotate = tf.random.uniform([],seed=seed)\n","    if p_rotate > .75:\n","        image = tf.image.rot90(image, k=3) # rotate 270º\n","        mask = tf.image.rot90(mask, k=3)\n","    elif p_rotate > .5:\n","        image = tf.image.rot90(image, k=2) # rotate 180º\n","        mask = tf.image.rot90(mask, k=2)\n","    elif p_rotate > .25:\n","        image = tf.image.rot90(image, k=1) # rotate 90º\n","        mask = tf.image.rot90(mask, k=1)\n","        \n","    # Drop out. Only for the image with 3 bands\n","    # image = dropout(image, size, p=1.0, ct=10, sz=0.1)\n","                \n","    # mask = tf.math.round(mask[:, :, 0]) #without this the mask stays 256,256,1 which I need\n","    \n","    # reshape (for TPU)\n","    # image = tf.reshape(image, [size, size, image.shape[-1]])\n","    # mask = tf.reshape(mask, [size, size])\n","    \n","    return image, mask\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSWHwINs1z9u"},"source":["## Create data set from tfrec"]},{"cell_type":"code","metadata":{"id":"vFqvSRdZbhNP"},"source":["#it works...\n","AUTO = tf.data.AUTOTUNE\n","ignore_order = tf.data.Options()\n","ignore_order.experimental_deterministic = False\n","\n","# def get_training_dataset_mix(files_ls,files_mix, num_repeat,len_shuffle,seed=P['SEED'], batch_size=n_batch):\n","#     dataset = tf.data.TFRecordDataset(files_mix, num_parallel_reads=AUTO)\n","#     dataset = dataset.with_options(ignore_order)\n","#     dataset = dataset.cache()\n","\n","#     #repeat the ls dataset to mitigate class imbalance\n","#     dataset_ls = tf.data.TFRecordDataset(files_ls, num_parallel_reads=AUTO)\n","#     dataset_ls = dataset_ls.with_options(ignore_order)\n","#     dataset_ls = dataset_ls.cache()\n","#     dataset_ls = dataset_ls.repeat(num_repeat) \n","\n","#     #merge the two datasets #not sure if concatenate is the proper way\n","#     ds = dataset.concatenate(dataset_ls)\n","\n","#     #then do map of decode_record and augment. By augment the oversampled images become less duplicative.\n","#     ds = ds.map(lambda ex: decode_record(ex), num_parallel_calls=AUTO)\n","#     ds = ds.map(augment, num_parallel_calls=AUTO)\n","#     ds = ds.shuffle(buffer_size= len_shuffle, seed=seed) #when shuffle?? #shuffle before batching?..  \n","#     ds = ds.batch(batch_size)#, drop_remainder=True) #bc seconde one is only 16 for bsize 512.\n","#     ds = ds.prefetch(AUTO)\n","    \n","#     return ds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RE-ACrjTcaws"},"source":["\n","def get_training_dataset(files,len_shuffle,seed=P['SEED'], batch_size=n_batch):\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    dataset = dataset.with_options(ignore_order)\n","    dataset = dataset.cache()\n","    dataset = dataset.repeat(2) \n","    #then do map of decode_record and augment. By augment the oversampled images become less duplicative.\n","    # dataset = dataset.map(lambda ex: decode_record(ex), num_parallel_calls=AUTO)\n","    # dataset = dataset.map(scale_to_0_1,num_parallel_calls = AUTO) #or it could be included in decode_record function.\n","    dataset = dataset.map(decode_record, num_parallel_calls=AUTO)\n","    dataset = dataset.map(augment, num_parallel_calls=AUTO)\n","    dataset = dataset.shuffle(buffer_size= len_shuffle, seed=seed) #when shuffle?? #shuffle before batching?..  \n","    # dataset = dataset.batch(batch_size, drop_remainder=False) #bc seconde one is only 16 for bsize 512.\n","    dataset = dataset.batch(batch_size, drop_remainder=True) #bc seconde one is only 16 for bsize 512.\n","    dataset = dataset.prefetch(AUTO)\n","    \n","    return dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTXzSYE8pxit"},"source":["def get_validation_dataset(files, len_shuffle,ordered=True,seed=P['SEED'], batch_size=n_batch):\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n","    dataset = dataset.with_options(ignore_order)\n","    dataset = dataset.cache()\n","    # dataset = dataset.map(lambda ex: decode_record(ex), num_parallel_calls=AUTO)\n","    dataset = dataset.map(decode_record, num_parallel_calls=AUTO)\n","    dataset = dataset.shuffle(buffer_size= len_shuffle, seed=seed) \n","    dataset = dataset.batch(batch_size, drop_remainder=False)\n","    dataset = dataset.prefetch(AUTO)\n","\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdbGCQw_KU9L"},"source":["## get Kaggle gs, create ds"]},{"cell_type":"markdown","metadata":{"id":"qWOH-_par1p7"},"source":["for training set"]},{"cell_type":"code","metadata":{"id":"1Zoh5bHkjnJ4"},"source":["\n","train_dataset = get_training_dataset(tfrec_files_train,len_files_to_shuffle)\n","val_dataset = get_validation_dataset(tfrec_files_val,len_files_val_to_shuffle)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcBa5IBXlTHo"},"source":["## Check the (tfrecord) images"]},{"cell_type":"code","metadata":{"id":"YsJF3fVW6nRo"},"source":["# def check_shp(ds):\n","#   for [x1,x2] in ds.take(1):\n","#     print(x1.shape,x2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQ2jQqt76w7V"},"source":["# check_shp(val_dataset) #how its supposed to be right..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CL81qMwa6sB5"},"source":["# check_shp(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLcV5ETelV_H"},"source":["# # #maybe also dif between -1,1 and 0,1\n","\n","# # def check_y(ds):\n","# #   for [x1,x2] in ds.take(1):\n","# #     tar = x2[:,:,:,0]\n","# #     # print(tar.shape[0])\n","# #     # for i in range(tar.shape[0]):\n","# #     for i in range(10):\n","# #       # print('the x image',x1[i])\n","# #       # print('the y image',x2[i])\n","# #       #isnt it weird that max and min are not 0 and 1 for y, and 0 for x while 0.8 max .\n","# #       print('min',np.min(x1[i],axis=None),'and max of x ',np.max(x1[i],axis=None),'min ',np.min(x2[i],axis=None),'and max y ',np.max(x2[i],axis=None))\n","# #       print(tar[i].shape)\n","# #       plt.imshow(tar[i])\n","# #       plt.axis('off')\n","# #       plt.show()\n","# #       plt.close()\n","\n","# #dropout in every image?? Lets try without first\n","# def check_y(ds):\n","#   for [x1,x2] in ds.take(1):\n","#     tar = x2[:,:,:,0]\n","#     b1 = x1[:,:,:,0]\n","#     b2 = x1[:,:,:,1]\n","#     dem = x1[:,:,:,2]\n","#     print(tar.shape[0])\n","#     # for i in range(tar.shape[0]):\n","#     for i in range(10):\n","#       plt.imshow(b1[i])\n","#       plt.axis('off')\n","#       plt.show()\n","#       plt.close()\n","#       plt.imshow(b2[i])\n","#       plt.axis('off')\n","#       plt.show()\n","#       plt.close()\n","#       plt.imshow(dem[i])\n","#       plt.axis('off')\n","#       plt.show()\n","#       plt.close()\n","#       plt.imshow(tar[i])\n","#       plt.axis('off')\n","#       plt.show()\n","#       plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U9lKySKYo8O8"},"source":["Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1. \n","\n","Then why is y not scaled to 0,1 but does it look like 0.2,0.8 ~"]},{"cell_type":"markdown","metadata":{"id":"ScIz6hUd0Z4y"},"source":["On train min is 0.0 all the time (so from -1) on val its not, more like -0.2\n","\n","Og train min sar is -1.03 (check!), on val min sar is -1.03"]},{"cell_type":"code","metadata":{"id":"qWTKzZGF1VEd"},"source":["# check_y(val_ds_gan)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNR94yaQ1TS2"},"source":["# check_y(train_ds_gan)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlSE2ZoQ0O-S"},"source":["# check_y(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwz5QGG_moiK"},"source":["# check_y(val_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w0CG-1KpzV4J"},"source":["Ok so not sure if its bad that is 0.26, 0.73 instead of full 0,1 but at least it has mean 0.5 so that can be used as threshold\n","And y images look (seemingly) same as with -1,1"]},{"cell_type":"markdown","metadata":{"id":"mlvKzQZcdsvp"},"source":["# UNet"]},{"cell_type":"markdown","metadata":{"id":"WBehTZFudvpL"},"source":["https://keras.io/examples/vision/oxford_pets_image_segmentation/"]},{"cell_type":"code","metadata":{"id":"-Jod8pGrdujj"},"source":["from tensorflow.keras import layers\n","import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSfr3bRaedzr"},"source":["img_size = (256,256)\n","num_classes = 1 #bc then output is None,w,h,1. However arent there two classes in the sense that you have ls and non ls. Also sigmoid gives error when you put 1 instead of 2 O now it doesnt..\n","\n","# (512, 256, 256, 3) (512, 256, 256, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW9J_HwveYW7"},"source":["But then for sigmoid I need to scale all my values to [0,1] instead of -1,1 / -1,0.8 \n","--> possible to do a map function on tfrecs? Instead of writing them, again?"]},{"cell_type":"code","metadata":{"id":"fJ3U1NWPd8tK"},"source":["def get_model(img_size, num_classes):\n","    inputs = keras.Input(shape=img_size + (3,))\n","\n","    ### [First half of the network: downsampling inputs] ###\n","\n","    # Entry block\n","    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","\n","    previous_block_activation = x  # Set aside residual\n","\n","    # Blocks 1, 2, 3 are identical apart from the feature depth.\n","    for filters in [64, 128, 256]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n","\n","        # Project residual\n","        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n","            previous_block_activation\n","        )\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    ### [Second half of the network: upsampling inputs] ###\n","\n","    for filters in [256, 128, 64, 32]:\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.Activation(\"relu\")(x)\n","        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.UpSampling2D(2)(x)\n","\n","        # Project residual\n","        residual = layers.UpSampling2D(2)(previous_block_activation)\n","        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n","        x = layers.add([x, residual])  # Add back residual\n","        previous_block_activation = x  # Set aside next residual\n","\n","    # Add a per-pixel classification layer\n","    # The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression\n","    # outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n","    outputs = layers.Conv2D(num_classes, 3, activation=\"sigmoid\", padding=\"same\")(x) #when put sigmoid instead weird error if num_clases = 1. If 2 then fine. Its 2 bc nonls, ls\n","\n","\n","    # Define the model\n","    model = keras.Model(inputs, outputs)\n","    return model\n","\n","\n","# Free up RAM in case the model definition cells were run multiple times\n","keras.backend.clear_session()\n","\n","# Build model\n","model = get_model(img_size, num_classes)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEYoG6u6Jdj5"},"source":["# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60Wr1mwFSbwo"},"source":["from keras import backend as K\n","\n","def castF(x):\n","    return K.cast(x, K.floatx())\n","\n","def castB(x):\n","    return K.cast(x, bool)\n","\n","def iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n","    intersection = true * pred\n","    notTrue = 1 - true\n","    union = true + (notTrue * pred)\n","\n","    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n","\n","def competitionMetric2(true, pred): #any shape can go - can't be a loss function\n","\n","    tresholds = [0.5 + (i*.05)  for i in range(10)]\n","\n","    #flattened images (batch, pixels)\n","    true = K.batch_flatten(true)\n","    pred = K.batch_flatten(pred)\n","    pred = castF(K.greater(pred, 0.5))\n","\n","    #total white pixels - (batch,)\n","    trueSum = K.sum(true, axis=-1)\n","    predSum = K.sum(pred, axis=-1)\n","\n","    #has mask or not per image - (batch,)\n","    true1 = castF(K.greater(trueSum, 1))    \n","    pred1 = castF(K.greater(predSum, 1))\n","\n","    #to get images that have mask in both true and pred\n","    truePositiveMask = castB(true1 * pred1)\n","\n","    #separating only the possible true positives to check iou\n","    testTrue = tf.boolean_mask(true, truePositiveMask)\n","    testPred = tf.boolean_mask(pred, truePositiveMask)\n","\n","    #getting iou and threshold comparisons\n","    iou = iou_loss_core(testTrue,testPred) \n","    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n","\n","    #mean of thressholds for true positives and total sum\n","    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n","    truePositives = K.sum(truePositives)\n","\n","    #to get images that don't have mask in both true and pred\n","    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n","    trueNegatives = K.sum(trueNegatives) \n","\n","    return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n","\n","\n","#Just compile the model with metrics=[competitionMetric2]."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"H397OK2S9PH2","outputId":"b59fdb57-d0ca-41c7-b5a7-83bd2ddb1646"},"source":["# Configure the model for training.\n","\n","\n","# Optimizer Adam, loss naar binary cross entropy of focal dice loss entropy\n","# model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n","# model.compile(optimizer = \"Adam\",loss = bce_dice_loss)\n","model.compile(optimizer = \"Adam\",loss = bce_dice_loss,metrics=competitionMetric2)#[tf.keras.metrics.MeanIoU(num_classes=2)])\n","\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"unet_all_ls_asc_and_des.h5\", save_best_only=True) #big ls is act. as all ls.\n","]\n","\n","# callbacks = [\n","#     keras.callbacks.ModelCheckpoint(filepath = path, save_best_only=True),\n","#     tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/Thesis/Results/{name}.csv\", separator=\",\", append=False)\n","\n","# Train the model, doing validation at the end of each epoch.\n","# epochs = 100\n","# history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=callbacks)\n","history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, callbacks=callbacks)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","4/4 [==============================] - 215s 53s/step - loss: 4.6552 - competitionMetric2: 1.9531e-04 - val_loss: 1.3957 - val_competitionMetric2: 0.0023\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/100\n","4/4 [==============================] - 209s 53s/step - loss: 2.5759 - competitionMetric2: 0.0000e+00 - val_loss: 1.3098 - val_competitionMetric2: 0.0000e+00\n","Epoch 3/100\n","4/4 [==============================] - 203s 51s/step - loss: 1.5778 - competitionMetric2: 0.0000e+00 - val_loss: 1.2930 - val_competitionMetric2: 0.0000e+00\n","Epoch 4/100\n","4/4 [==============================] - 207s 52s/step - loss: 1.5256 - competitionMetric2: 1.9531e-04 - val_loss: 1.2933 - val_competitionMetric2: 0.0000e+00\n","Epoch 5/100\n","4/4 [==============================] - 206s 52s/step - loss: 1.3386 - competitionMetric2: 0.0000e+00 - val_loss: 1.2941 - val_competitionMetric2: 0.0000e+00\n","Epoch 6/100\n","4/4 [==============================] - 208s 52s/step - loss: 1.3340 - competitionMetric2: 0.0000e+00 - val_loss: 1.2959 - val_competitionMetric2: 0.0000e+00\n","Epoch 7/100\n","4/4 [==============================] - 208s 53s/step - loss: 1.3217 - competitionMetric2: 0.0000e+00 - val_loss: 1.2970 - val_competitionMetric2: 0.0000e+00\n","Epoch 8/100\n","4/4 [==============================] - 208s 52s/step - loss: 1.3047 - competitionMetric2: 0.0000e+00 - val_loss: 1.2968 - val_competitionMetric2: 0.0000e+00\n","Epoch 9/100\n","4/4 [==============================] - 209s 53s/step - loss: 1.3053 - competitionMetric2: 0.0000e+00 - val_loss: 1.2979 - val_competitionMetric2: 0.0000e+00\n","Epoch 10/100\n","4/4 [==============================] - 208s 52s/step - loss: 1.3047 - competitionMetric2: 0.0000e+00 - val_loss: 1.2983 - val_competitionMetric2: 0.0000e+00\n","Epoch 11/100\n","4/4 [==============================] - 208s 53s/step - loss: 1.3011 - competitionMetric2: 0.0000e+00 - val_loss: 1.2984 - val_competitionMetric2: 0.0000e+00\n","Epoch 12/100\n","4/4 [==============================] - 208s 53s/step - loss: 1.2981 - competitionMetric2: 0.0000e+00 - val_loss: 1.2967 - val_competitionMetric2: 0.0000e+00\n","Epoch 13/100\n","4/4 [==============================] - 208s 52s/step - loss: 1.2979 - competitionMetric2: 0.0000e+00 - val_loss: 1.2954 - val_competitionMetric2: 0.0000e+00\n","Epoch 14/100\n","4/4 [==============================] - 208s 53s/step - loss: 1.2980 - competitionMetric2: 0.0000e+00 - val_loss: 1.2958 - val_competitionMetric2: 0.0000e+00\n","Epoch 15/100\n","4/4 [==============================] - 205s 52s/step - loss: 1.2971 - competitionMetric2: 0.0000e+00 - val_loss: 1.2956 - val_competitionMetric2: 0.0000e+00\n","Epoch 16/100\n","4/4 [==============================] - 207s 52s/step - loss: 1.2964 - competitionMetric2: 0.0000e+00 - val_loss: 1.2957 - val_competitionMetric2: 0.0000e+00\n","Epoch 17/100\n","4/4 [==============================] - 206s 52s/step - loss: 1.2962 - competitionMetric2: 0.0000e+00 - val_loss: 1.2961 - val_competitionMetric2: 0.0000e+00\n","Epoch 18/100\n","1/4 [======>.......................] - ETA: 2:30 - loss: 1.2962 - competitionMetric2: 0.0000e+00"]}]},{"cell_type":"markdown","metadata":{"id":"vx1BCAz3Q0rC"},"source":["Mean IoU class does not have a threshold..\n","tf.keras.metrics.Recall() does have threshold of 0.5"]},{"cell_type":"code","metadata":{"id":"WVfTnR6gF8hU"},"source":["#learning curves\n","# acc = history.history['accuracy']\n","# val_acc = history.history['val_accuracy']\n","name = 'all_ls_asc_des_model'\n","acc = history.history['competitionMetric2']\n","val_acc = history.history['val_competitionMetric2']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(num_epochs)\n","\n","plt.figure(figsize=(10, 5), dpi=200)\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training')\n","plt.plot(epochs_range, val_acc, label='Validation')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation IoU')\n","plt.xlabel('Epoch')\n","plt.ylabel('IoU')\n","plt.ylim(0, 1.05)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training')\n","plt.plot(epochs_range, val_loss, label='Validation')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","# plt.ylim(0, 0.8)\n","plt.savefig(path_learning_curve + name + '_curve.png')\n","plt.tight_layout()\n","plt.show()\n","plt.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8cE46k5e6bT"},"source":["#learning curve?\n","# model.history()\n","\n","\n","# I had the exact same error when building my own tfrecords to retrain my model. The issue was that the height of one of the labeled boxes was negative. I'd recommend checking the sanity of your data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPck22OBfNWj"},"source":["# Predictions"]},{"cell_type":"code","metadata":{"id":"Z8NOhFvQfOu5"},"source":["# Generate predictions for all images in the validation set\n","\n","val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n","val_preds = model.predict(val_gen)\n","\n","\n","def display_mask(i):\n","    \"\"\"Quick utility to display a model's prediction.\"\"\"\n","    mask = np.argmax(val_preds[i], axis=-1)\n","    mask = np.expand_dims(mask, axis=-1)\n","    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n","    display(img)\n","\n","\n","# Display results for validation image #10\n","i = 10\n","\n","# Display input image\n","display(Image(filename=val_input_img_paths[i]))\n","\n","# Display ground-truth target mask\n","img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n","display(img)\n","\n","# Display mask predicted by our model\n","display_mask(i)  # Note that the model only sees inputs at 150x150."],"execution_count":null,"outputs":[]}]}